"""
è´¢ç»æ–°é—»å’Œæ”¿ç­–ä¿¡æ¯æ”¶é›†æ¨¡å—
æ”¯æŒ LLM è¯­ä¹‰åŒ–æ–°é—»åˆ†æ
"""

from datetime import datetime, timedelta
from typing import Optional
import json
import akshare as ak
import pandas as pd

# LLM API é…ç½®
LLM_API_KEY = "24bW7BzhYaf5O"
LLM_BASE_URL = "https://ai.liaobots.work/v1"
LLM_MODEL = "gpt-4o"


def llm_analyze_news(news_list: list[dict], sector_flow: list[dict] = None) -> dict:
    """
    ä½¿ç”¨ LLM å¯¹æ–°é—»è¿›è¡Œè¯­ä¹‰åŒ–åˆ†æ

    è¿”å›:
    {
        'analyzed_news': [
            {
                'title': æ–°é—»æ ‡é¢˜,
                'importance': 0-10 é‡è¦æ€§è¯„åˆ†,
                'sentiment': -1åˆ°1 æƒ…æ„Ÿå€¾å‘,
                'sectors': ['åŠå¯¼ä½“', 'æ¶ˆè´¹ç”µå­'],  # å—å½±å“æ¿å—
                'reason': 'è¯„åˆ†ç†ç”±'
            }
        ],
        'overall_sentiment': æ•´ä½“å¸‚åœºæƒ…ç»ª (-1åˆ°1),
        'hot_sectors': ['æ¿å—1', 'æ¿å—2'],  # ä»Šæ—¥çƒ­ç‚¹æ¿å—
        'resonance': [  # æ–°é—»ä¸èµ„é‡‘å…±æŒ¯
            {'sector': 'åŠå¯¼ä½“', 'news_sentiment': 0.8, 'capital_flow': 142.79, 'resonance': True}
        ]
    }
    """
    try:
        from openai import OpenAI
    except ImportError:
        print("  âš  æœªå®‰è£… openai åº“ï¼Œè·³è¿‡ LLM åˆ†æ")
        return {'error': 'openai not installed'}

    if not news_list:
        return {'analyzed_news': [], 'overall_sentiment': 0, 'hot_sectors': [], 'resonance': []}

    # å‡†å¤‡æ–°é—»æ–‡æœ¬
    news_texts = []
    for i, news in enumerate(news_list[:20]):  # æœ€å¤šåˆ†æ20æ¡
        title = news.get('title', news.get('content', ''))[:100]
        source = news.get('source', '')
        news_texts.append(f"{i+1}. [{source}] {title}")

    news_block = "\n".join(news_texts)

    # å‡†å¤‡èµ„é‡‘æµå‘ä¿¡æ¯ï¼ˆç”¨äºå…±æŒ¯åˆ†æï¼‰
    flow_info = ""
    top_inflow_sectors = []
    top_outflow_sectors = []
    if sector_flow:
        # åˆ†ç¦»æµå…¥å’Œæµå‡º
        inflow_texts = []
        outflow_texts = []
        for sector in sector_flow:
            name = sector.get('name', '')
            net_flow = sector.get('net_flow', sector.get('net_inflow', 0))
            sector_type = sector.get('type', '')
            if sector_type == 'inflow' or net_flow > 0:
                inflow_texts.append(f"- {name}: +{abs(net_flow):.2f}äº¿")
                top_inflow_sectors.append(name)
            else:
                outflow_texts.append(f"- {name}: -{abs(net_flow):.2f}äº¿")
                top_outflow_sectors.append(name)

        flow_info = "\n\nğŸ“Š ä»Šæ—¥è¡Œä¸šèµ„é‡‘æµå‘:"
        if inflow_texts:
            flow_info += f"\nä¸»åŠ›å‡€æµå…¥:\n" + "\n".join(inflow_texts[:5])
        if outflow_texts:
            flow_info += f"\n\nä¸»åŠ›å‡€æµå‡º:\n" + "\n".join(outflow_texts[:5])

    # æ„å»ºå…±æŒ¯åˆ†æè¦æ±‚
    resonance_instruction = ""
    if top_inflow_sectors:
        resonance_instruction = f"""

ğŸ” å…±æŒ¯åˆ†æä»»åŠ¡:
è¯·å¯¹æ¯”ä»Šæ—¥èµ„é‡‘æµå…¥å‰äº”çš„è¡Œä¸šï¼ˆ{', '.join(top_inflow_sectors[:5])}ï¼‰ä¸ä»Šæ—¥è¦é—»:
1. è‹¥æ–°é—»ä¸­æœ‰å…³äºè¿™äº›è¡Œä¸šçš„é‡å¤§åˆ©å¥½ï¼Œæ ‡æ³¨ä¸º"é€»è¾‘å…±æŒ¯"ï¼ˆè¶‹åŠ¿æ›´æŒä¹…ï¼‰
2. è‹¥è¡Œä¸šèµ„é‡‘å¤§æ¶¨ä½†æ— ç›¸å…³æ–°é—»æ”¯æ’‘ï¼Œæ ‡æ³¨ä¸º"èµ„é‡‘é©±åŠ¨"ï¼ˆæ˜“å›è°ƒï¼‰
3. è‹¥æ–°é—»åˆ©ç©ºä½†èµ„é‡‘ä»æµå…¥ï¼Œæ ‡æ³¨ä¸º"åˆ©ç©ºä¸è·Œ"ï¼ˆä¸»åŠ›æ‰˜åº•ï¼‰"""

    prompt = f"""ä½œä¸ºèµ„æ·±æŠ•èµ„ç»ç†ï¼Œè¯·åˆ†æä»¥ä¸‹è´¢ç»æ–°é—»ï¼Œè¿”å› JSON æ ¼å¼ç»“æœã€‚

ä»Šæ—¥æ–°é—»åˆ—è¡¨:
{news_block}
{flow_info}
{resonance_instruction}

è¯·è¿”å›ä¸¥æ ¼çš„ JSON æ ¼å¼ï¼ˆä¸è¦æœ‰å…¶ä»–æ–‡å­—ï¼‰:
{{
    "analyzed_news": [
        {{
            "index": 1,
            "importance": 8,
            "sentiment": 0.5,
            "sectors": ["åŠå¯¼ä½“", "æ¶ˆè´¹ç”µå­"],
            "reason": "ç®€çŸ­ç†ç”±"
        }}
    ],
    "overall_sentiment": 0.3,
    "hot_sectors": ["åŠå¯¼ä½“", "æ–°èƒ½æº"],
    "resonance": [
        {{
            "sector": "åŠå¯¼ä½“",
            "news_sentiment": 0.8,
            "capital_flow": 142.79,
            "type": "é€»è¾‘å…±æŒ¯",
            "conclusion": "é€»è¾‘å…±æŒ¯ï¼šæ–°é—»åˆ©å¥½(AIèŠ¯ç‰‡éœ€æ±‚)+èµ„é‡‘æµå…¥142äº¿ï¼Œè¶‹åŠ¿è¾ƒæŒä¹…"
        }},
        {{
            "sector": "è½¯ä»¶å¼€å‘",
            "news_sentiment": 0,
            "capital_flow": -90.73,
            "type": "èµ„é‡‘é©±åŠ¨",
            "conclusion": "èµ„é‡‘é©±åŠ¨ï¼šæ— æ˜æ˜¾æ–°é—»æ”¯æ’‘ä½†èµ„é‡‘å¤§å¹…æµå‡ºï¼Œè°¨æ…è§‚æœ›"
        }}
    ],
    "market_summary": "ä¸€å¥è¯æ€»ç»“ä»Šæ—¥å¸‚åœºæƒ…ç»ªå’Œæ“ä½œå»ºè®®"
}}

è¯„åˆ†æ ‡å‡†:
- importance (0-10): 0=ç¤¼ä»ªæ€§æ–°é—», 5=è¡Œä¸šä¸€èˆ¬æ¶ˆæ¯, 8=é‡å¤§æ”¿ç­–, 10=å½±å“å…¨å¸‚åœº
- sentiment (-1åˆ°1): -1=æå¤§åˆ©ç©º, 0=ä¸­æ€§, 1=æå¤§åˆ©å¥½
- sectors: åªå¡«å†™ç›´æ¥å—å½±å“çš„Aè‚¡æ¿å—åç§°
- resonance: å¿…é¡»é’ˆå¯¹èµ„é‡‘æµå…¥/æµå‡ºå‰5çš„è¡Œä¸šé€ä¸€åˆ†æï¼Œæ ‡æ³¨ä¸ºé€»è¾‘å…±æŒ¯ã€èµ„é‡‘é©±åŠ¨æˆ–åˆ©ç©ºä¸è·Œ"""

    try:
        client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)

        response = client.chat.completions.create(
            model=LLM_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„Aè‚¡æŠ•èµ„åˆ†æå¸ˆï¼Œæ“…é•¿ä»æ–°é—»ä¸­æå–æŠ•èµ„ä¿¡å·ã€‚åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=2000
        )

        result_text = response.choices[0].message.content.strip()

        # å°è¯•è§£æ JSON
        # å¤„ç†å¯èƒ½çš„ markdown ä»£ç å—
        if result_text.startswith("```"):
            result_text = result_text.split("```")[1]
            if result_text.startswith("json"):
                result_text = result_text[4:]
        result_text = result_text.strip()

        result = json.loads(result_text)

        # å°†åˆ†æç»“æœåˆå¹¶å›åŸæ–°é—»åˆ—è¡¨
        analyzed_map = {item['index']: item for item in result.get('analyzed_news', [])}
        for i, news in enumerate(news_list[:20]):
            if i + 1 in analyzed_map:
                analysis = analyzed_map[i + 1]
                news['llm_importance'] = analysis.get('importance', 5)
                news['llm_sentiment'] = analysis.get('sentiment', 0)
                news['llm_sectors'] = analysis.get('sectors', [])
                news['llm_reason'] = analysis.get('reason', '')

        return {
            'analyzed_news': news_list[:20],
            'overall_sentiment': result.get('overall_sentiment', 0),
            'hot_sectors': result.get('hot_sectors', []),
            'resonance': result.get('resonance', []),
            'market_summary': result.get('market_summary', '')
        }

    except json.JSONDecodeError as e:
        print(f"  âš  LLM è¿”å›æ ¼å¼é”™è¯¯: {e}")
        return {'error': f'JSON parse error: {e}'}
    except Exception as e:
        print(f"  âš  LLM åˆ†æå¤±è´¥: {e}")
        return {'error': str(e)}


def llm_analyze_cctv_news(cctv_news: list[dict]) -> dict:
    """
    ä¸“é—¨é’ˆå¯¹æ–°é—»è”æ’­çš„æ”¿ç­–å¯¼å‘åˆ†æ

    è¿”å›:
    {
        'policy_signals': [
            {'topic': 'äº§ä¸šæ”¿ç­–', 'direction': 'åˆ©å¥½', 'sectors': ['æ–°èƒ½æº'], 'term': 'ä¸­é•¿æœŸ'}
        ],
        'key_meetings': ['å›½åŠ¡é™¢å¸¸åŠ¡ä¼šè®®', 'ä¸­å¤®æ”¿æ²»å±€ä¼šè®®'],
        'investment_hints': 'æŠ•èµ„å¯ç¤º'
    }
    """
    try:
        from openai import OpenAI
    except ImportError:
        return {'error': 'openai not installed'}

    if not cctv_news:
        return {'policy_signals': [], 'key_meetings': [], 'investment_hints': ''}

    # åˆå¹¶æ–°é—»è”æ’­å†…å®¹
    cctv_texts = []
    for news in cctv_news[:5]:
        title = news.get('title', '')
        content = news.get('content', '')[:300]
        cctv_texts.append(f"æ ‡é¢˜: {title}\næ‘˜è¦: {content}")

    cctv_block = "\n\n".join(cctv_texts)

    prompt = f"""ä½œä¸ºæ”¿ç­–åˆ†æå¸ˆï¼Œè¯·è§£è¯»ä»Šæ—¥ã€Šæ–°é—»è”æ’­ã€‹çš„æŠ•èµ„ä¿¡å·ã€‚

æ–°é—»è”æ’­å†…å®¹:
{cctv_block}

è¯·è¿”å›ä¸¥æ ¼çš„ JSON æ ¼å¼:
{{
    "policy_signals": [
        {{
            "topic": "æ”¿ç­–ä¸»é¢˜",
            "direction": "åˆ©å¥½/åˆ©ç©º/ä¸­æ€§",
            "sectors": ["å—ç›Šæ¿å—1", "å—ç›Šæ¿å—2"],
            "term": "çŸ­æœŸ/ä¸­æœŸ/é•¿æœŸ",
            "reason": "ç®€è¦åˆ†æ"
        }}
    ],
    "key_meetings": ["æåˆ°çš„é‡è¦ä¼šè®®åç§°"],
    "investment_hints": "ä¸€å¥è¯æŠ•èµ„å¯ç¤º"
}}

é‡ç‚¹å…³æ³¨:
- å›½åŠ¡é™¢å¸¸åŠ¡ä¼šè®®ã€ä¸­å¤®æ”¿æ²»å±€ã€æ·±åŒ–æ”¹é©ç­‰å…³é”®è¯
- äº§ä¸šæ”¿ç­–å¯¼å‘ï¼ˆå¦‚æ–°èƒ½æºã€åŠå¯¼ä½“ã€æ¶ˆè´¹ï¼‰
- å¿½ç•¥å¤–äº¤ç¤¼ä»ªã€ä½“è‚²æ–‡åŒ–ç­‰éç»æµæ–°é—»"""

    try:
        client = OpenAI(api_key=LLM_API_KEY, base_url=LLM_BASE_URL)

        response = client.chat.completions.create(
            model=LLM_MODEL,
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„å®è§‚æ”¿ç­–åˆ†æå¸ˆã€‚åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )

        result_text = response.choices[0].message.content.strip()
        if result_text.startswith("```"):
            result_text = result_text.split("```")[1]
            if result_text.startswith("json"):
                result_text = result_text[4:]
        result_text = result_text.strip()

        return json.loads(result_text)

    except Exception as e:
        print(f"  âš  æ–°é—»è”æ’­åˆ†æå¤±è´¥: {e}")
        return {'error': str(e)}


def get_cctv_news() -> list[dict]:
    """
    è·å–æ–°é—»è”æ’­æ–‡å­—ç¨¿ - é‡å¤§æ”¿ç­–é£å‘æ ‡

    è¿”å›æœ€è¿‘çš„æ–°é—»è”æ’­å†…å®¹æ‘˜è¦
    """
    try:
        df = ak.news_cctv(date=datetime.now().strftime("%Y%m%d"))
        if df is not None and not df.empty:
            news_list = []
            for _, row in df.head(10).iterrows():
                news_list.append({
                    'title': row.get('title', ''),
                    'content': row.get('content', '')[:200] + '...' if len(str(row.get('content', ''))) > 200 else row.get('content', ''),
                    'source': 'æ–°é—»è”æ’­',
                    'date': datetime.now().strftime("%Y-%m-%d")
                })
            return news_list
    except Exception as e:
        print(f"  è·å–æ–°é—»è”æ’­å¤±è´¥: {e}")
    return []


def get_eastmoney_news() -> list[dict]:
    """
    è·å–ä¸œæ–¹è´¢å¯Œè´¢ç»è¦é—»

    è¿”å›ä»Šæ—¥é‡è¦è´¢ç»æ–°é—»
    """
    try:
        df = ak.stock_info_global_em()
        if df is not None and not df.empty:
            news_list = []
            for _, row in df.head(15).iterrows():
                title = row.get('æ ‡é¢˜', '')
                if not title:
                    continue
                news_list.append({
                    'title': title,
                    'summary': row.get('æ‘˜è¦', ''),
                    'time': str(row.get('å‘å¸ƒæ—¶é—´', '')),
                    'source': 'ä¸œæ–¹è´¢å¯Œ',
                })
            return news_list
    except Exception as e:
        print(f"  è·å–ä¸œæ–¹è´¢å¯Œæ–°é—»å¤±è´¥: {e}")
    return []


def get_cls_telegraph() -> list[dict]:
    """
    è·å–è´¢è”ç¤¾ç”µæŠ¥ - å¿«è®¯æ¶ˆæ¯

    è¿”å›æœ€æ–°è´¢ç»å¿«è®¯
    """
    try:
        df = ak.stock_info_global_cls()
        if df is not None and not df.empty:
            news_list = []
            for _, row in df.head(20).iterrows():
                title = row.get('æ ‡é¢˜', '')
                content = row.get('å†…å®¹', '')
                if not title and not content:
                    continue

                # ç»„åˆæ—¥æœŸå’Œæ—¶é—´
                pub_date = str(row.get('å‘å¸ƒæ—¥æœŸ', ''))
                pub_time = str(row.get('å‘å¸ƒæ—¶é—´', ''))
                time_str = f"{pub_date} {pub_time}".strip()

                news_list.append({
                    'title': title if title else content[:50] + '...' if len(content) > 50 else content,
                    'content': content[:150] + '...' if len(str(content)) > 150 else content,
                    'time': time_str,
                    'source': 'è´¢è”ç¤¾',
                })
            return news_list
    except Exception as e:
        print(f"  è·å–è´¢è”ç¤¾ç”µæŠ¥å¤±è´¥: {e}")
    return []


def get_jin10_news() -> list[dict]:
    """
    è·å–é‡‘åæ•°æ®å¿«è®¯

    è¿”å›é‡‘èå¸‚åœºå®æ—¶èµ„è®¯
    """
    try:
        df = ak.js_news(timestamp=datetime.now().strftime("%Y%m%d%H%M%S"))
        if df is not None and not df.empty:
            news_list = []
            for _, row in df.head(15).iterrows():
                content = row.get('content', '')
                if content:
                    news_list.append({
                        'content': content[:200] + '...' if len(content) > 200 else content,
                        'time': row.get('time', ''),
                        'source': 'é‡‘åæ•°æ®',
                    })
            return news_list
    except Exception as e:
        print(f"  è·å–é‡‘åæ•°æ®å¤±è´¥: {e}")
    return []


def get_macro_china_news() -> list[dict]:
    """
    è·å–ä¸­å›½å®è§‚ç»æµæ•°æ®å‘å¸ƒ

    è¿”å›æœ€è¿‘çš„å®è§‚æ•°æ®ï¼ˆä¼˜å…ˆä½¿ç”¨æœˆåº¦/å­£åº¦æ•°æ®æ¥å£ï¼‰
    """
    news_list = []

    # GDP (å­£åº¦æ•°æ®ï¼Œæ¯å­£é¦–æœˆ15å·å·¦å³å‘å¸ƒ)
    try:
        # å°è¯•è·å–å­£åº¦GDPæ•°æ®
        gdp = ak.macro_china_gdp()
        if gdp is not None and not gdp.empty:
            # ç¡®ä¿æŒ‰æ—¥æœŸæ’åºå–æœ€æ–°æ•°æ®
            date_col = 'æ—¥æœŸ' if 'æ—¥æœŸ' in gdp.columns else 'å­£åº¦' if 'å­£åº¦' in gdp.columns else None
            if date_col:
                gdp = gdp.sort_values(date_col, ascending=True)
            latest = gdp.iloc[-1]
            value = latest.get('ä»Šå€¼', latest.get('å›½å†…ç”Ÿäº§æ€»å€¼-åŒæ¯”å¢é•¿', 'N/A'))
            date = str(latest.get('æ—¥æœŸ', latest.get('å­£åº¦', 'N/A')))
            news_list.append({
                'title': f"GDPåŒæ¯”: {value}%",
                'content': f"æ•°æ®æœŸ: {date}",
                'source': 'å®è§‚æ•°æ®',
                'type': 'GDP'
            })
    except Exception as e:
        # é™çº§åˆ°å¹´åº¦æ•°æ®
        try:
            gdp = ak.macro_china_gdp_yearly()
            if gdp is not None and not gdp.empty:
                # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
                if 'æ—¥æœŸ' in gdp.columns:
                    gdp = gdp.sort_values('æ—¥æœŸ', ascending=True)
                valid_gdp = gdp[gdp['ä»Šå€¼'].notna()]
                if not valid_gdp.empty:
                    latest = valid_gdp.iloc[-1]
                    value = latest.get('ä»Šå€¼', 'N/A')
                    date = str(latest.get('æ—¥æœŸ', 'N/A'))
                    news_list.append({
                        'title': f"GDPåŒæ¯”: {value}%",
                        'content': f"å‘å¸ƒæ—¥æœŸ: {date}",
                        'source': 'å®è§‚æ•°æ®',
                        'type': 'GDP'
                    })
        except Exception as e2:
            print(f"  è·å–GDPæ•°æ®å¤±è´¥: {e2}")

    # CPI (æœˆåº¦æ•°æ®ï¼Œæ¯æœˆ10å·å·¦å³å‘å¸ƒ)
    try:
        cpi = ak.macro_china_cpi()
        if cpi is not None and not cpi.empty:
            # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
            date_col = 'æ—¥æœŸ' if 'æ—¥æœŸ' in cpi.columns else 'æœˆä»½' if 'æœˆä»½' in cpi.columns else None
            if date_col:
                cpi = cpi.sort_values(date_col, ascending=True)
            latest = cpi.iloc[-1]
            value = latest.get('ä»Šå€¼', latest.get('å…¨å›½-åŒæ¯”å¢é•¿', 'N/A'))
            date = str(latest.get('æ—¥æœŸ', latest.get('æœˆä»½', 'N/A')))
            news_list.append({
                'title': f"CPIåŒæ¯”: {value}%",
                'content': f"æ•°æ®æœŸ: {date}",
                'source': 'å®è§‚æ•°æ®',
                'type': 'CPI'
            })
    except Exception as e:
        # é™çº§åˆ°å¹´åº¦æ•°æ®
        try:
            cpi = ak.macro_china_cpi_yearly()
            if cpi is not None and not cpi.empty:
                # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
                if 'æ—¥æœŸ' in cpi.columns:
                    cpi = cpi.sort_values('æ—¥æœŸ', ascending=True)
                valid_cpi = cpi[cpi['ä»Šå€¼'].notna()]
                if not valid_cpi.empty:
                    latest = valid_cpi.iloc[-1]
                    value = latest.get('ä»Šå€¼', 'N/A')
                    date = str(latest.get('æ—¥æœŸ', 'N/A'))
                    news_list.append({
                        'title': f"CPIåŒæ¯”: {value}%",
                        'content': f"å‘å¸ƒæ—¥æœŸ: {date}",
                        'source': 'å®è§‚æ•°æ®',
                        'type': 'CPI'
                    })
        except Exception as e2:
            print(f"  è·å–CPIæ•°æ®å¤±è´¥: {e2}")

    # PPI (æœˆåº¦æ•°æ®)
    try:
        ppi = ak.macro_china_ppi()
        if ppi is not None and not ppi.empty:
            # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
            date_col = 'æ—¥æœŸ' if 'æ—¥æœŸ' in ppi.columns else 'æœˆä»½' if 'æœˆä»½' in ppi.columns else None
            if date_col:
                ppi = ppi.sort_values(date_col, ascending=True)
            latest = ppi.iloc[-1]
            value = latest.get('ä»Šå€¼', latest.get('å·¥ä¸šå“å‡ºå‚ä»·æ ¼æŒ‡æ•°-åŒæ¯”å¢é•¿', 'N/A'))
            date = str(latest.get('æ—¥æœŸ', latest.get('æœˆä»½', 'N/A')))
            news_list.append({
                'title': f"PPIåŒæ¯”: {value}%",
                'content': f"æ•°æ®æœŸ: {date}",
                'source': 'å®è§‚æ•°æ®',
                'type': 'PPI'
            })
    except Exception as e:
        pass  # PPI éå¿…é¡»

    # PMI (æœˆåº¦æ•°æ®ï¼Œæ¯æœˆæœ€åä¸€å¤©å‘å¸ƒ)
    try:
        pmi = ak.macro_china_pmi()
        if pmi is not None and not pmi.empty:
            # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
            date_col = 'æ—¥æœŸ' if 'æ—¥æœŸ' in pmi.columns else 'æœˆä»½' if 'æœˆä»½' in pmi.columns else None
            if date_col:
                pmi = pmi.sort_values(date_col, ascending=True)
            latest = pmi.iloc[-1]
            value = latest.get('ä»Šå€¼', latest.get('åˆ¶é€ ä¸š-æŒ‡æ•°', 'N/A'))
            date = str(latest.get('æ—¥æœŸ', latest.get('æœˆä»½', 'N/A')))
            try:
                status = "æ‰©å¼ " if float(value) > 50 else "æ”¶ç¼©"
            except:
                status = ""
            news_list.append({
                'title': f"åˆ¶é€ ä¸šPMI: {value}" + (f" ({status})" if status else ""),
                'content': f"æ•°æ®æœŸ: {date}",
                'source': 'å®è§‚æ•°æ®',
                'type': 'PMI'
            })
    except Exception as e:
        # é™çº§åˆ°å¹´åº¦æ•°æ®
        try:
            pmi = ak.macro_china_pmi_yearly()
            if pmi is not None and not pmi.empty:
                # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
                if 'æ—¥æœŸ' in pmi.columns:
                    pmi = pmi.sort_values('æ—¥æœŸ', ascending=True)
                valid_pmi = pmi[pmi['ä»Šå€¼'].notna()]
                if not valid_pmi.empty:
                    latest = valid_pmi.iloc[-1]
                    value = latest.get('ä»Šå€¼', 'N/A')
                    date = str(latest.get('æ—¥æœŸ', 'N/A'))
                    status = "æ‰©å¼ " if float(value) > 50 else "æ”¶ç¼©"
                    news_list.append({
                        'title': f"åˆ¶é€ ä¸šPMI: {value} ({status})",
                        'content': f"å‘å¸ƒæ—¥æœŸ: {date}",
                        'source': 'å®è§‚æ•°æ®',
                        'type': 'PMI'
                    })
        except Exception as e2:
            print(f"  è·å–PMIæ•°æ®å¤±è´¥: {e2}")

    # ç¤¾èæ•°æ® (æœˆåº¦)
    try:
        sf = ak.macro_china_shrzgm()
        if sf is not None and not sf.empty:
            # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
            date_col = 'æœˆä»½' if 'æœˆä»½' in sf.columns else 'æ—¥æœŸ' if 'æ—¥æœŸ' in sf.columns else None
            if date_col:
                sf = sf.sort_values(date_col, ascending=True)
            latest = sf.iloc[-1]
            value = latest.get('ç¤¾ä¼šèèµ„è§„æ¨¡å¢é‡', 'N/A')
            date = str(latest.get('æœˆä»½', 'N/A'))
            news_list.append({
                'title': f"ç¤¾èå¢é‡: {value}äº¿",
                'content': f"æ•°æ®æœŸ: {date}",
                'source': 'å®è§‚æ•°æ®',
                'type': 'SHRZGM'
            })
    except Exception as e:
        pass  # ç¤¾èéå¿…é¡»

    return news_list


def get_us_economic_calendar() -> list[dict]:
    """
    è·å–ç¾å›½ç»æµæ—¥å† - é‡è¦ç»æµæ•°æ®å‘å¸ƒ

    è¿”å›è¿‘æœŸç¾å›½ç»æµæ•°æ®
    """
    import math
    news_list = []

    try:
        df = ak.macro_usa_cpi_monthly()
        if df is not None and not df.empty:
            # æ‰¾æœ€è¿‘ä¸€æ¡æœ‰æ•ˆæ•°æ®
            valid_df = df[df['ä»Šå€¼'].notna()]
            if not valid_df.empty:
                latest = valid_df.iloc[-1]
                value = latest.get('ä»Šå€¼', 'N/A')
                prev = latest.get('å‰å€¼', 'N/A')
                date = str(latest.get('æ—¥æœŸ', 'N/A'))
                news_list.append({
                    'title': f"ç¾å›½CPIæœˆç‡: {value}%",
                    'content': f"å‰å€¼: {prev}% | å‘å¸ƒ: {date}",
                    'source': 'ç¾å›½ç»æµæ•°æ®',
                    'type': 'US_CPI'
                })
    except Exception as e:
        print(f"  è·å–ç¾å›½CPIæ•°æ®å¤±è´¥: {e}")

    return news_list


def filter_important_news(news_list: list[dict], keywords: list[str] = None) -> list[dict]:
    """
    ç­›é€‰é‡è¦æ–°é—»

    keywords: å…³é”®è¯åˆ—è¡¨ï¼ŒåŒ…å«è¿™äº›è¯çš„æ–°é—»ä¼šè¢«æ ‡è®°ä¸ºé‡è¦
    """
    if keywords is None:
        keywords = [
            # æ”¿ç­–ç›¸å…³
            'å¤®è¡Œ', 'é™æ¯', 'é™å‡†', 'åŠ æ¯', 'LPR', 'è´§å¸æ”¿ç­–', 'è´¢æ”¿æ”¿ç­–',
            'è¯ç›‘ä¼š', 'å‘æ”¹å§”', 'å›½åŠ¡é™¢', 'æ”¿æ²»å±€',
            # å¸‚åœºç›¸å…³
            'åŒ—å‘èµ„é‡‘', 'å¤–èµ„', 'ä¸»åŠ›èµ„é‡‘', 'èèµ„', 'èåˆ¸',
            # è¡Œä¸šç›¸å…³
            'èŠ¯ç‰‡', 'åŠå¯¼ä½“', 'æ–°èƒ½æº', 'äººå·¥æ™ºèƒ½', 'AI', 'å…‰ä¼', 'é”‚ç”µ',
            # ç¾å›½ç›¸å…³
            'ç¾è”å‚¨', 'Fed', 'ç¾è‚¡', 'çº³æ–¯è¾¾å…‹', 'æ ‡æ™®',
            # é‡å¤§äº‹ä»¶
            'æš´è·Œ', 'æš´æ¶¨', 'ç†”æ–­', 'é»‘å¤©é¹…', 'åˆ©å¥½', 'åˆ©ç©º',
        ]

    important = []
    normal = []

    for news in news_list:
        text = str(news.get('title', '')) + str(news.get('content', ''))
        is_important = any(kw in text for kw in keywords)

        if is_important:
            news['important'] = True
            important.append(news)
        else:
            news['important'] = False
            normal.append(news)

    # é‡è¦æ–°é—»åœ¨å‰
    return important + normal


def collect_daily_news(sector_flow: list[dict] = None, use_llm: bool = True) -> dict:
    """
    æ”¶é›†æ¯æ—¥è´¢ç»æ–°é—»æ±‡æ€»

    å‚æ•°:
        sector_flow: è¡Œä¸šèµ„é‡‘æµå‘æ•°æ®ï¼Œç”¨äº LLM å…±æŒ¯åˆ†æ
        use_llm: æ˜¯å¦ä½¿ç”¨ LLM è¿›è¡Œè¯­ä¹‰åˆ†æ

    è¿”å›: {
        'cctv': æ–°é—»è”æ’­,
        'telegraph': è´¢è”ç¤¾/é‡‘åå¿«è®¯,
        'macro': å®è§‚æ•°æ®,
        'all_news': æ‰€æœ‰æ–°é—»åˆå¹¶å¹¶æŒ‰é‡è¦æ€§æ’åº,
        'important_count': é‡è¦æ–°é—»æ•°é‡,
        'llm_analysis': LLM åˆ†æç»“æœ,
        'cctv_analysis': æ–°é—»è”æ’­æ”¿ç­–åˆ†æ,
        'updated_at': æ›´æ–°æ—¶é—´
    }
    """
    print("ğŸ“° æ”¶é›†è´¢ç»æ–°é—»...")

    result = {
        'cctv': [],
        'telegraph': [],
        'eastmoney': [],
        'macro': [],
        'all_news': [],
        'important_count': 0,
        'llm_analysis': {},
        'cctv_analysis': {},
        'updated_at': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }

    # æ–°é—»è”æ’­
    print("  è·å–æ–°é—»è”æ’­...")
    result['cctv'] = get_cctv_news()

    # è´¢è”ç¤¾å¿«è®¯
    print("  è·å–è´¢è”ç¤¾å¿«è®¯...")
    result['telegraph'] = get_cls_telegraph()

    # ä¸œæ–¹è´¢å¯Œè¦é—»
    print("  è·å–ä¸œæ–¹è´¢å¯Œè¦é—»...")
    result['eastmoney'] = get_eastmoney_news()

    # å®è§‚æ•°æ®
    print("  è·å–å®è§‚ç»æµæ•°æ®...")
    result['macro'] = get_macro_china_news()
    result['macro'].extend(get_us_economic_calendar())

    # åˆå¹¶æ‰€æœ‰æ–°é—»
    all_news = []
    all_news.extend(result['cctv'])
    all_news.extend(result['telegraph'])
    all_news.extend(result['eastmoney'])

    # ç­›é€‰é‡è¦æ–°é—»ï¼ˆå…³é”®è¯åˆç­›ï¼‰
    result['all_news'] = filter_important_news(all_news)
    result['important_count'] = sum(1 for n in result['all_news'] if n.get('important'))

    print(f"  âœ“ å…±è·å– {len(result['all_news'])} æ¡æ–°é—»ï¼Œå…¶ä¸­é‡è¦ {result['important_count']} æ¡")

    # LLM è¯­ä¹‰åˆ†æ
    if use_llm:
        print("  ğŸ¤– ä½¿ç”¨ LLM åˆ†ææ–°é—»æƒ…ç»ª...")

        # åˆ†æä¸€èˆ¬æ–°é—»
        llm_result = llm_analyze_news(result['all_news'], sector_flow)
        if 'error' not in llm_result:
            result['llm_analysis'] = llm_result
            # æŒ‰ LLM é‡è¦æ€§é‡æ–°æ’åº
            result['all_news'] = sorted(
                result['all_news'],
                key=lambda x: x.get('llm_importance', 0),
                reverse=True
            )
            print(f"  âœ“ LLM åˆ†æå®Œæˆï¼Œæ•´ä½“æƒ…ç»ª: {llm_result.get('overall_sentiment', 0):+.2f}")

            # æ˜¾ç¤ºå…±æŒ¯ä¿¡æ¯
            resonance = llm_result.get('resonance', [])
            if resonance:
                for r in resonance[:3]:
                    print(f"    ğŸ“Š {r.get('conclusion', '')}")
        else:
            print(f"  âš  LLM åˆ†æè·³è¿‡: {llm_result.get('error', '')}")

        # åˆ†ææ–°é—»è”æ’­æ”¿ç­–å¯¼å‘
        if result['cctv']:
            print("  ğŸ›ï¸ åˆ†ææ–°é—»è”æ’­æ”¿ç­–ä¿¡å·...")
            cctv_result = llm_analyze_cctv_news(result['cctv'])
            if 'error' not in cctv_result:
                result['cctv_analysis'] = cctv_result
                hints = cctv_result.get('investment_hints', '')
                if hints:
                    print(f"  âœ“ æ”¿ç­–å¯ç¤º: {hints}")

    return result


def format_news_for_report(news_data: dict, max_items: int = 10) -> str:
    """
    å°†æ–°é—»æ•°æ®æ ¼å¼åŒ–ä¸ºæŠ¥å‘Šæ–‡æœ¬ï¼ˆæ”¯æŒ LLM åˆ†æç»“æœï¼‰
    """
    lines = ["## ä»Šæ—¥è¦é—»\n"]

    # LLM å¸‚åœºæƒ…ç»ªæ€»ç»“
    llm_analysis = news_data.get('llm_analysis', {})
    if llm_analysis and 'error' not in llm_analysis:
        overall = llm_analysis.get('overall_sentiment', 0)
        summary = llm_analysis.get('market_summary', '')

        # æƒ…ç»ªå›¾æ ‡
        if overall > 0.3:
            emoji = "ğŸŸ¢"
            mood = "åå¤š"
        elif overall < -0.3:
            emoji = "ğŸ”´"
            mood = "åç©º"
        else:
            emoji = "ğŸŸ¡"
            mood = "ä¸­æ€§"

        lines.append("### ğŸ“Š AI å¸‚åœºæƒ…ç»ªåˆ†æ\n")
        lines.append(f"- æ•´ä½“æƒ…ç»ª: {emoji} **{mood}** ({overall:+.2f})")
        if summary:
            lines.append(f"- ä»Šæ—¥æ€»ç»“: {summary}")

        # çƒ­ç‚¹æ¿å—
        hot_sectors = llm_analysis.get('hot_sectors', [])
        if hot_sectors:
            lines.append(f"- çƒ­ç‚¹æ¿å—: **{', '.join(hot_sectors[:5])}**")

        # æ–°é—»ä¸èµ„é‡‘å…±æŒ¯
        resonance = llm_analysis.get('resonance', [])
        if resonance:
            lines.append("\n**é€»è¾‘å…±æŒ¯:**\n")
            for r in resonance[:3]:
                conclusion = r.get('conclusion', '')
                if conclusion:
                    lines.append(f"- {conclusion}")
        lines.append("")

    # æ–°é—»è”æ’­æ”¿ç­–åˆ†æ
    cctv_analysis = news_data.get('cctv_analysis', {})
    if cctv_analysis and 'error' not in cctv_analysis:
        policy_signals = cctv_analysis.get('policy_signals', [])
        hints = cctv_analysis.get('investment_hints', '')

        if policy_signals or hints:
            lines.append("### ğŸ›ï¸ æ”¿ç­–ä¿¡å·\n")

            for signal in policy_signals[:3]:
                topic = signal.get('topic', '')
                direction = signal.get('direction', '')
                sectors = signal.get('sectors', [])
                term = signal.get('term', '')

                direction_emoji = "ğŸŸ¢" if direction == "åˆ©å¥½" else ("ğŸ”´" if direction == "åˆ©ç©º" else "ğŸŸ¡")
                sectors_str = f"â†’ {', '.join(sectors)}" if sectors else ""
                lines.append(f"- {direction_emoji} **{topic}** ({term}) {sectors_str}")

            if hints:
                lines.append(f"\n> ğŸ’¡ {hints}")
            lines.append("")

    # å®è§‚æ•°æ®
    if news_data.get('macro'):
        lines.append("### ğŸ“ˆ å®è§‚æ•°æ®\n")
        for item in news_data['macro']:
            lines.append(f"- **{item.get('title', '')}** {item.get('content', '')}")
        lines.append("")

    # é‡è¦æ–°é—»ï¼ˆæŒ‰ LLM é‡è¦æ€§æ’åºï¼‰
    all_news = news_data.get('all_news', [])

    # å¦‚æœæœ‰ LLM åˆ†æï¼ŒæŒ‰é‡è¦æ€§ç­›é€‰
    if llm_analysis and 'error' not in llm_analysis:
        high_importance = [n for n in all_news if n.get('llm_importance', 0) >= 6]
        if high_importance:
            lines.append("### ğŸ“° é«˜ä»·å€¼èµ„è®¯\n")
            lines.append("| æ¥æº | æ–°é—» | é‡è¦æ€§ | æƒ…ç»ª | ç›¸å…³æ¿å— |")
            lines.append("|------|------|--------|------|----------|")

            for item in high_importance[:max_items]:
                title = item.get('title', item.get('content', ''))[:40]
                source = item.get('source', '')
                importance = item.get('llm_importance', 0)
                sentiment = item.get('llm_sentiment', 0)
                sectors = item.get('llm_sectors', [])

                # æƒ…ç»ªå›¾æ ‡
                sent_emoji = "ğŸŸ¢" if sentiment > 0.2 else ("ğŸ”´" if sentiment < -0.2 else "ğŸŸ¡")
                sectors_str = ', '.join(sectors[:2]) if sectors else "-"

                lines.append(f"| {source} | {title} | {importance}/10 | {sent_emoji} {sentiment:+.1f} | {sectors_str} |")
            lines.append("")
    else:
        # æ—  LLM æ—¶ï¼Œä½¿ç”¨å…³é”®è¯ç­›é€‰
        important_news = [n for n in all_news if n.get('important')]
        if important_news:
            lines.append("### é‡è¦èµ„è®¯\n")
            for item in important_news[:max_items]:
                title = item.get('title', item.get('content', ''))[:60]
                source = item.get('source', '')
                time = item.get('time', '')
                lines.append(f"- [{source}] {title}")
                if time:
                    lines.append(f"  - æ—¶é—´: {time}")
            lines.append("")

    # æ–°é—»è”æ’­æ‘˜è¦ï¼ˆæ—  LLM åˆ†ææ—¶æ˜¾ç¤ºï¼‰
    if news_data.get('cctv') and not cctv_analysis:
        lines.append("### æ–°é—»è”æ’­è¦ç‚¹\n")
        for item in news_data['cctv'][:3]:
            title = item.get('title', '')
            if title:
                lines.append(f"- {title}")
        lines.append("")

    if not all_news and not news_data.get('macro') and not news_data.get('cctv'):
        lines.append("æš‚æ— é‡è¦æ–°é—»\n")

    return "\n".join(lines)


if __name__ == "__main__":
    news_data = collect_daily_news()

    print("\n" + "=" * 50)
    print(format_news_for_report(news_data))
